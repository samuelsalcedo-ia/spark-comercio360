# Comercio360: Pipeline Big Data con Apache Spark en AWS

Este proyecto implementa una arquitectura de procesamiento de datos distribuida y escalable utilizando **Apache Spark** sobre infraestructura **AWS EC2**. El objetivo es procesar hist√≥ricos de ventas (ETL), calcular m√©tricas de negocio complejas y persistir los resultados en una base de datos **RDS MySQL**.

## üöÄ Arquitectura del Cl√∫ster

El despliegue se ha realizado en AWS siguiendo las mejores pr√°cticas de separaci√≥n de roles y seguridad:

* **Infraestructura de C√≥mputo (EC2):**
    * **Master Node:** `t2.medium` (Orquestador del cl√∫ster).
    * **Worker Nodes (x3):** `t2.micro` (Procesamiento distribuido).
    * **Submit Node:** `t2.micro` (Cliente/Basti√≥n para lanzamiento de jobs).
* **Almacenamiento:**
    * **Data Lake:** Amazon S3 (Datos crudos CSV).
    * **Base de Datos Operacional:** Amazon RDS (MySQL 8.0) para la capa de servicio.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

* **Lenguaje:** Python 3 (PySpark).
* **Motor:** Apache Spark 3.5.1.
* **Base de Datos:** MySQL 8.0 (AWS RDS).
* **Infraestructura como C√≥digo (IaC):** Bash scripting para aprovisionamiento.
* **Librer√≠as Clave:** `mysql-connector-java`, `hadoop-aws`.
* **DevOps:** Despliegue automatizado mediante Git/GitHub y gesti√≥n de secretos con Variables de Entorno.

## üìÇ Estructura del Proyecto

* `setup_ec2.sh`: **Script de Aprovisionamiento (IaC)**. Instala Java, Spark, Python, Git y descarga autom√°ticamente los drivers necesarios (MySQL/AWS).
* `job_analytics_completo.py`: **Script Principal ETL**. Realiza la ingesta desde S3, transformaciones (Joins, Window Functions) y carga en RDS.
* `consultar_resultados_sql.py`: Script de auditor√≠a que conecta a RDS y muestra las tablas resultantes por consola para verificar la persistencia.
* `requirements.txt`: Lista de dependencias de Python.
* `README.md`: Documentaci√≥n oficial del proyecto.

## üìä L√≥gica de Negocio (Consultas)

El pipeline resuelve tres necesidades anal√≠ticas cr√≠ticas:

1.  **Consulta A (Top Productos):** Ranking diario de los 10 productos con mayor facturaci√≥n.
2.  **Consulta B (KPIs Mensuales):** Agregaci√≥n mensual calculando clientes √∫nicos, ticket medio y total de ventas por categor√≠a.
3.  **Consulta C (Detecci√≥n de Outliers):** Algoritmo estad√≠stico avanzado usando Ventanas Deslizantes (*Rolling Windows*) para detectar d√≠as con ventas an√≥malas (superiores a la media de los 30 d√≠as previos + 2 desviaciones est√°ndar).

## ‚öôÔ∏è Instalaci√≥n y Despliegue

### 1. Aprovisionamiento de Infraestructura (User Data)
El proyecto incluye un script de automatizaci√≥n (`setup_ec2.sh`) que prepara el entorno.

Para desplegar un nuevo nodo en AWS EC2:
1.  Lanzar instancia (Ubuntu 22.04).
2.  En la secci√≥n **Advanced Details** -> **User Data**, pegar el contenido de `setup_ec2.sh`.
3.  Al iniciar, la m√°quina tendr√° Spark, Git y los Drivers configurados autom√°ticamente.

### 2. Configuraci√≥n del Cl√∫ster (Arranque Manual)
Una vez aprovisionados los nodos, es necesario iniciar los demonios de Spark y conectar los Workers al Master:

**En el Nodo Master:**
```bash
# Iniciar el proceso maestro
/opt/spark/sbin/start-master.sh
# Nota: Copiar la URL del log (ej: spark://ip-172-31-XX-XX:7077)

```

**En cada Nodo Worker (x3):**

```bash
# Conectar el worker al maestro
/opt/spark/sbin/start-worker.sh spark://<IP-PRIVADA-MASTER>:7077

```

### 3. Clonar el Repositorio

En el nodo Submit (Cliente), descargamos el c√≥digo fuente:

```bash
git clone [https://github.com/samuelsalcedo-ia/spark-comercio360.git](https://github.com/samuelsalcedo-ia/spark-comercio360.git)
cd spark-comercio360

```

### 4. Configuraci√≥n de Seguridad

Por seguridad, **no** incluimos credenciales en el c√≥digo. Define la contrase√±a de la base de datos como variable de entorno antes de ejecutar:

```bash
export DB_PASSWORD='[PASSWORD]'

```

### 5. Ejecuci√≥n del Pipeline ETL

Lanzamos el trabajo al cl√∫ster en modo cliente. Se han ajustado los par√°metros de memoria para optimizar el rendimiento en instancias `t2.micro`:

```bash
/opt/spark/bin/spark-submit \
  --master spark://<IP-PRIVADA-MASTER>:7077 \
  --deploy-mode client \
  --executor-memory 512M \
  --driver-memory 512M \
  --conf spark.executor.cores=1 \
  --conf spark.cores.max=3 \
  --driver-class-path /opt/spark/jars/mysql-connector-java-8.0.28.jar \
  --jars /opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/mysql-connector-java-8.0.28.jar \
  job_analytics_completo.py

```

### 6. Verificaci√≥n de Resultados (Auditor√≠a)

Para confirmar que los datos se han guardado correctamente en MySQL, ejecutamos el script de validaci√≥n que consulta directamente a la base de datos:

```bash
/opt/spark/bin/spark-submit \
  --driver-class-path /opt/spark/jars/mysql-connector-java-8.0.28.jar \
  --jars /opt/spark/jars/mysql-connector-java-8.0.28.jar \
  consultar_resultados_sql.py

```