# üõí Comercio360: Pipeline Big Data con Apache Spark en AWS

Este proyecto implementa una arquitectura de procesamiento de datos distribuida y escalable utilizando **Apache Spark** sobre infraestructura **AWS EC2**. El objetivo es procesar hist√≥ricos de ventas (ETL), calcular m√©tricas de negocio complejas y persistir los resultados en una base de datos **RDS MySQL**.

## üöÄ Arquitectura del Cl√∫ster

El despliegue se ha realizado en AWS siguiendo las mejores pr√°cticas de separaci√≥n de roles y seguridad:

* **Infraestructura de C√≥mputo (EC2):**
    * **Master Node:** `t2.medium` (Orquestador del cl√∫ster).
    * **Worker Nodes (x3):** `t2.micro` (Procesamiento distribuido).
    * **Submit Node:** `t2.micro` (Cliente/Basti√≥n para lanzamiento de jobs).
* **Almacenamiento:**
    * **Data Lake:** Amazon S3 (Datos crudos CSV).
    * **Base de Datos Operacional:** Amazon RDS (MySQL 8.0) para la capa de servicio.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

* **Lenguaje:** Python 3 (PySpark).
* **Motor:** Apache Spark 3.5.1.
* **Base de Datos:** MySQL 8.0 (AWS RDS).
* **Librer√≠as Clave:** `mysql-connector-java`, `hadoop-aws`.
* **DevOps:** Despliegue automatizado mediante Git/GitHub y gesti√≥n de secretos con Variables de Entorno.

## üìÇ Estructura del Proyecto

* `job_analytics_completo.py`: **Script Principal ETL**. Realiza la ingesta desde S3, transformaciones (Joins, Window Functions) y carga en RDS.
* `consultar_resultados_sql.py`: Script de verificaci√≥n que conecta a RDS y muestra las tablas resultantes por consola.
* `README.md`: Documentaci√≥n del proyecto.

## üìä L√≥gica de Negocio (Consultas)

El pipeline resuelve tres necesidades anal√≠ticas cr√≠ticas:

1.  **Consulta A (Top Productos):** Ranking diario de los 10 productos con mayor facturaci√≥n.
2.  **Consulta B (KPIs Mensuales):** Agregaci√≥n mensual calculando clientes √∫nicos, ticket medio y total de ventas por categor√≠a.
3.  **Consulta C (Detecci√≥n de Outliers):** Algoritmo estad√≠stico avanzado usando Ventanas Deslizantes (*Rolling Windows*) para detectar d√≠as con ventas an√≥malas (superiores a la media de los 30 d√≠as previos + 2 desviaciones est√°ndar).

## ‚öôÔ∏è Instalaci√≥n y Despliegue

### 1. Prerrequisitos
* Tener acceso al cl√∫ster Spark en AWS.
* Tener las librer√≠as de conexi√≥n (`mysql-connector-java` y `hadoop-aws`) en `/opt/spark/jars`.

### 2. Clonar el Repositorio
En el nodo Submit (Cliente):
```bash
git clone [https://github.com/samuelsalcedo-ia/spark-comercio360.git](https://github.com/samuelsalcedo-ia/spark-comercio360.git)
cd spark-comercio360
